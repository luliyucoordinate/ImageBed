---
layout: post
title: 目标检测中的mAP计算
category : 机器学习
tags : [mAP, python]
stickie: true
date: 2018-04-09 00:00:00
---

# 0x00 基本概念

在说明`mAP`这个概念之前，我们先要说一下两个新的概念：**precision**和**recall**。这是论文中经常提及的两个概念。我们举这样的一个例子：

我们现在有一个检测`狗`的任务，但是这个任务数据中包含了`猫`。那么我们这个时候就有一个问题，我们怎么描述一个物体被检测出来呢？为此，我们引入了`IOU`这个概念

[目标检测中的IOU计算问题](https://blog.csdn.net/qq_17550379/article/details/78815637)

我们规定`IOU > 0.5`表示物体被检测出来，否则没有。

<center class="half">
    <img src="https://raw.githubusercontent.com/wiki/luliyucoordinate/ImageBed/mAP/2019_6_25_1.jpg" width="600">
</center>


现在我们有了这个指标，我们开始跑数据。假设我们已经把网络训练好了，接着找来一张图片测试（这张图片上有四个`狗`和三只`猫`），测试结果这样的（我们的目标是找图片中的`狗`）

<center class="half">
    <img src="https://raw.githubusercontent.com/wiki/luliyucoordinate/ImageBed/mAP/2019_6_25_2.jpg" width="500">
</center>

标注的数字表示`IOU`值，我们只选择`IOU > 0.5`的物体。

那么，这个时候问题又出现了，我们怎么描述我们检测的结果怎么样呢？这个时候就出现了**precision**和**recall**。我们先不急着说明这两个概念是什么，我们先看上面的结果，我们发现，我们要找的目标是`狗`，结果查找的目标中有`猫`，这显然不是我们希望看到的。我们希望通过一个指标去描述这个问题，所以我们提出了**precision**，**precision**是描述查找一个目标的精准率。我们还发现一个问题，就是我们漏找了，明明右上角是一只`狗`，结果没有找出来，为了表述这个问题，所以我们提出**recall**，**recall**是描述查找一个目标的漏检率，**recall**很多地方翻译为召回率，我更喜欢称它为漏检率或查全率。

我们将被正确识别的`狗`，称为**True positives**。我们将被正确识别的`猫`称为**True negatives**。为什么会有这个概念？什么是被正确识别的`猫`？我们知道我们这里的目标是找`狗`，那么那些我们没有标注的`猫`，是不是从反向说明我们的查找准确率（**precision**）呢？接着，我们定义被错误识别为`狗`的`猫`为**False positives**。被错误识别为`猫`的`狗`称为**False negatives**（就是右上角的狗，受`IOU`指标影响）。这些概念很重要，不要记错了！！！

接着我们就定义

- $precision=\frac{True positives}{True positives+False positives}$
- $recall=\frac{Truepositives}{Truepositives+Falsenegatives}$

我们就很容易的计算出这里的$precision=\frac{3}{4}$，$recall=\frac{3}{4}$ 。最后说一点，如果我们把`IOU`标准设置为`IOU > 0`的话，这个时候的**recall**会越来越接近于`1`，而相应的**precision**也会降低，这很容易理解。

另外还有一个概念**accuracy**

- $accuracy=\frac{Truepositives+Truenegatives}{Truepositives+Falsenegatives+Truenegatives+Falsepositives}$

我们这里很容易计算出$accuracy=\frac{3+1}{3+1+2+1}=\frac{4}{7}$。最后放一张很经典的图片

<center class="half">
    <img src="https://raw.githubusercontent.com/wiki/luliyucoordinate/ImageBed/mAP/2019_6_25_3.jpg" width="400">
</center>

# 0x01 代码

我们这里使用的评估脚本是按照`VOC`数据集的标准来的，预测结果按如下格式：

- `<image identifier> <confidence> <left> <top> <right> <buttom>`

`VOC`数据集总共有20类，也就是说，我们最后要得到20个结果文件。例如，对于`car`，输出结果

```
comp3_det_test_car.txt:
	000004 0.702732 89 112 516 466
```

得到这些文件后，我们可以通过以下脚本，计算对应的`mAP`

[![GitHub forks](https://img.shields.io/github/forks/badges/shields.svg?style=social&label=Fork)](https://github.com/luliyucoordinate/eval_voc)

我这里简单提一下脚本中的AP计算问题

```python
def voc_ap(rec, prec, use_07_metric=False):
    if use_07_metric:
        # 11 point metric
        ap = 0.
        for t in np.arange(0., 1.1, 0.1):
            if np.sum(rec >= t) == 0:
                p = 0
            else:
                p = np.max(prec[rec >= t])
            ap = ap + p / 11.
    else:
        mrec = np.concatenate(([0.], rec, [1.]))
        mpre = np.concatenate(([0.], prec, [0.]))
        for i in range(mpre.size - 1, 0, -1):
            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])
        i = np.where(mrec[1:] != mrec[:-1])[0]
        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap
```

我们注意到这里的`AP`计算有两种

- 11点插值法，这个算法的思路很简单。就是选取0,0.1,0.2...1，这样的11个点，分别对应不同的**recall**级别，根据不同的级别计算最大**precision**，然后求出它的平均值。
- 第二种就是我们文中提到的方法

reference:

[The PASCAL Visual Object Classes Challenge (VOC2007)](http://www.pascal-network.org/challenges/VOC/voc2007/index.html)

如有任何问题，希望大家指出，谢谢O(∩_∩)O