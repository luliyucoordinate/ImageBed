---
layout: post
title: python快速读取非常大的文件
category : python
tags : [python, big file]
stickie: true
date: 2018-05-06 00:00:00
---

读取大文件是我们平时经常会遇到的问题，我这里给出两个比较好的解决方案。第一种

```python
with open("test.txt") as f:
    for line in f:
        #do something with data
```

这种做法非常的简单。这个代码在打开文件的过程中，不会一次性读取全部文件，而是采用每次读取一行的方式，类似于`buffer`机制。

当然我们也可以自己去实现一个`buffer`，然后通过协程的方式操作

```python
def readInChunks(fileObj, chunkSize=4096):
    """
    Lazy function to read a file piece by piece.
    Default chunk size: 4kB.
    """
    while 1:
        data = fileObj.read(chunkSize)
        if not data:
            break
        yield data

f = open('bigFile')
for chuck in readInChunks(f):
    #do_something(chunk)
f.close()
```

这段代码中我们通过每次读取`4k`大小的数据，将所有文件读取完。

我对于一个`3GB`大小的数据进行了读取测试，分别用时如下：

```python
28.54150631145376 s
28.522545760074877 s
```

差不多。

